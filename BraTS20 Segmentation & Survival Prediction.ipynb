{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "rvPgudonTYn8",
        "T98BlHJFUe6S"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rvPgudonTYn8"
      },
      "source": [
        "# **Brain Tumor Segmentation**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FcV0uwQpTcTG"
      },
      "source": [
        "## **Libraries**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iv_lYhA_TSt3"
      },
      "source": [
        "import os\n",
        "import keras\n",
        "#!pip install nibabel\n",
        "import nibabel as nib\n",
        "from PIL import Image\n",
        "#!pip install sklearn\n",
        "from sklearn.utils import class_weight\n",
        "import tensorflow as tf\n",
        "\n",
        "import os\n",
        "from skimage.io import imread, imshow, concatenate_images\n",
        "from skimage.transform import resize\n",
        "# from medpy.io import load\n",
        "import numpy as np\n",
        "\n",
        "#import cv2\n",
        "import nibabel as nib\n",
        "from PIL import Image\n",
        "import random\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R75Kd37iUayV"
      },
      "source": [
        "## **Segmentation model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CDHjiNSLTiCW"
      },
      "source": [
        "\n",
        "def standardize(image):\n",
        "\n",
        "  standardized_image = np.zeros(image.shape)\n",
        "\n",
        "  #\n",
        "  \n",
        "      # iterate over the `z` dimension\n",
        "  for z in range(image.shape[2]):\n",
        "      # get a slice of the image \n",
        "      # at channel c and z-th dimension `z`\n",
        "      image_slice = image[:,:,z]\n",
        "\n",
        "      # subtract the mean from image_slice\n",
        "      centered = image_slice - np.mean(image_slice)\n",
        "      \n",
        "      # divide by the standard deviation (only if it is different from zero)\n",
        "      if(np.std(centered)!=0):\n",
        "          centered = centered/np.std(centered) \n",
        "\n",
        "      # update  the slice of standardized image\n",
        "      # with the scaled centered and scaled image\n",
        "      standardized_image[:, :, z] = centered\n",
        "\n",
        "  ### END CODE HERE ###\n",
        "\n",
        "  return standardized_image\n",
        "\n",
        "\n",
        "def dice_coef(y_true, y_pred, epsilon=0.00001):\n",
        "    \"\"\"\n",
        "    Dice = (2*|X & Y|)/ (|X|+ |Y|)\n",
        "         =  2*sum(|A*B|)/(sum(A^2)+sum(B^2))\n",
        "    ref: https://arxiv.org/pdf/1606.04797v1.pdf\n",
        "    \n",
        "    \"\"\"\n",
        "    axis = (0,1,2,3)\n",
        "    dice_numerator = 2. * K.sum(y_true * y_pred, axis=axis) + epsilon\n",
        "    dice_denominator = K.sum(y_true*y_true, axis=axis) + K.sum(y_pred*y_pred, axis=axis) + epsilon\n",
        "    return K.mean((dice_numerator)/(dice_denominator))\n",
        "\n",
        "def dice_coef_loss(y_true, y_pred):\n",
        "    return 1-dice_coef(y_true, y_pred)\n",
        "\n",
        "def DilatedConvolution3DBlock(input_mat,num_filters,kernel_size, channels, g, stride, batch_norm, dilation ):\n",
        "  X = tf.keras.layers.BatchNormalization()(input_mat)\n",
        "  X = tf.keras.layers.Conv3D(num_filters,kernel_size=(kernel_size,kernel_size,kernel_size),strides=(1,1,1),padding='same')(X)\n",
        "  X = tf.keras.layers.Activation('relu')(X)\n",
        "  return X\n",
        "\n",
        "def DMFUnit(input_mat,num_filters,kernel_size, channels, g, stride, batch_norm, dilation ):\n",
        "  X1 = DilatedConvolution3DBlock(input_mat,num_filters,kernel_size, channels, g, stride, batch_norm, dilation )\n",
        "  X2 = DilatedConvolution3DBlock(X1,num_filters,kernel_size, channels, g, stride, batch_norm, dilation )\n",
        "  X3 = DilatedConvolution3DBlock(X2,num_filters,kernel_size, channels, g, stride, batch_norm, dilation )\n",
        "  X = tf.keras.layers.concatenate([X1,X2,X3]);\n",
        "  print('X1',X1.shape)\n",
        "  print('X2',X2.shape)\n",
        "  print('X3',X3.shape)\n",
        "  print('X',X.shape)\n",
        "  X = DilatedConvolution3DBlock(X,num_filters,kernel_size, channels, g, stride, batch_norm, dilation )\n",
        "  print('X',X.shape)\n",
        "  return X\n",
        "def DMFUnit(input_mat,n_filters,kernel, channels, g, strides, batch_norm, dilation ):\n",
        "    batch_normalization=True\n",
        "    activation=False\n",
        "    print('input_layer',input_mat.shape)\n",
        "    layer1 = tf.keras.layers.Conv3D(n_filters, kernel, padding='same', strides=strides,dilation_rate=1)(input_mat)\n",
        "    print('layer1',layer1.shape)\n",
        "    if batch_normalization:\n",
        "        layer1 = tf.keras.layers.BatchNormalization(axis=1)(layer1)\n",
        "    elif instance_normalization:\n",
        "        # import tensorflow_addons as tfa\n",
        "        layer1 = tf.keras.layers.BatchNormalization(axis=1)(layer1)\n",
        "    if activation is None:\n",
        "        layer1= tf.keras.layers.Activation('relu')(layer1)\n",
        "    else:\n",
        "        layer1= activation()(layer1)\n",
        "    ######################### \n",
        "    layer2 = tf.keras.layers.Conv3D(n_filters, kernel, padding='same', strides=strides,dilation_rate=2)(input_layer)\n",
        "    if batch_normalization:\n",
        "        layer2 = tf.keras.layers.BatchNormalization(axis=1)(layer2)\n",
        "    elif instance_normalization:\n",
        "        # import tensorflow_addons as tfa\n",
        "        layer2 = tf.keras.layers.BatchNormalization(axis=1)(layer2)\n",
        "    if activation is None:\n",
        "        layer2= tf.keras.layers.Activation('relu')(layer2)\n",
        "    else:\n",
        "        layer2= activation()(layer2)\n",
        "    ######################### \n",
        "    layer3 = tf.keras.layers.Conv3D(n_filters, kernel, padding='same', strides=strides,dilation_rate=3)(input_layer)\n",
        "    if batch_normalization:\n",
        "        layer3 = tf.keras.layers.BatchNormalization(axis=1)(layer3)\n",
        "    elif instance_normalization:\n",
        "        # import tensorflow_addons as tfa\n",
        "        layer3 = tf.keras.layers.BatchNormalization(axis=1)(layer3)\n",
        "    if activation is None:\n",
        "        layer3= tf.keras.layers.Activation('relu')(layer3)\n",
        "    else:\n",
        "        layer3= activation()(layer3)\n",
        "    layer=tf.keras.layers.concatenate([layer1,layer2,layer3]);\n",
        "    print('layer1',layer1.shape)\n",
        "    print('layer2',layer2.shape)\n",
        "    print('layer3',layer3.shape)\n",
        "    print('layer',layer.shape)\n",
        "    layer = tf.keras.layers.Conv3D(n_filters, kernel, padding='same', strides=strides,dilation_rate=1)(layer)\n",
        "    print('layer',layer.shape)\n",
        "    return layer\n",
        "c=4,\n",
        "n=32,\n",
        "channels=128, \n",
        "groups=16,\n",
        "norm='bn',\n",
        "num_classes=4\n",
        "\n",
        "def MLDC(input_img, n_filters = 8, dropout = 0.2, batch_norm = True):\n",
        "   print('Input Image',input_img.shape)\n",
        "   c1=DMFUnit(input_img,32,3, channels, g=groups, strides=2, batch_norm=True,dilation=[1,2,3])\n",
        "   print('C1',c1.shape)\n",
        "   \n",
        "   p1 = tf.keras.layers.MaxPooling3D(pool_size=(2,2,2) ,strides=2,padding='valid')(c1)\n",
        "   print('p1',p1.shape)\n",
        "   c2=DMFUnit(p1,n_filters*2, 3, channels, g=groups, stride=2, batch_norm=True,dilation=[1,2,3]) \n",
        "   print('C2',c2.shape)\n",
        "\n",
        "   p2 = tf.keras.layers.MaxPooling3D(pool_size=(2,2,2) ,strides=2,padding='valid')(c2)\n",
        "   print('p2',p2.shape)\n",
        "   \n",
        "   c3=DMFUnit(p2,n_filters*4,3, channels, g=groups, stride=2, batch_norm=True,dilation=[1,2,3])\n",
        "   print('c3',c3.shape)\n",
        "  \n",
        "   u1 = tf.keras.layers.Conv3DTranspose(n_filters*4,(3,3,3),strides = (2,2,2) , padding='same')(c3)\n",
        "   print('u1',u1.shape)\n",
        "   con1 = tf.keras.layers.concatenate([u1,c2]);\n",
        "   print('con1',con1.shape)\n",
        "   c4 = DMFUnit(con1,n_filters*2, 3, channels, g=groups, stride=2, batch_norm=True,dilation=[1,2,3]) \n",
        "   print('c4',c4.shape)\n",
        "    \n",
        "   u2 = tf.keras.layers.Conv3DTranspose(n_filters*4,(3,3,3),strides = (2,2,2) , padding='same')(c4)\n",
        "   print('u2',u2.shape)\n",
        "   con2 = tf.keras.layers.concatenate([u2,c1]);\n",
        "   print('con2',con2.shape)\n",
        "   c5 = DMFUnit(con2,n_filters*2, 3, channels, g=groups, stride=2, batch_norm=True,dilation=[1,2,3]) \n",
        "   print('c5',c5.shape)\n",
        "   outputs = tf.keras.layers.Conv3D(4, (1, 1,1), activation='softmax')(c5)\n",
        "   print('outputs',outputs.shape)\n",
        "  #  print(outputs.shape)\n",
        "   model = tf.keras.Model(inputs=input_img, outputs=outputs)\n",
        "\n",
        "   return model\n",
        "\n",
        " \n",
        "input_img = tf.keras.Input((128,128,128,4))\n",
        "model = MLDC(input_img,8,0.1,True)\n",
        "learning_rate = 0.001\n",
        "epochs = 5000\n",
        "decay_rate = 0.0000001\n",
        "model.compile(optimizer=tf.keras.optimizers.Adam(lr=learning_rate, decay = decay_rate), loss=dice_coef_loss, metrics=[dice_coef,'acc'])\n",
        "model.summary()\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HDQ3LbigUWPo"
      },
      "source": [
        "## **Preprocessing**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zf4SqmGfTvqv"
      },
      "source": [
        "\n",
        "\n",
        "path = '/content/drive/MyDrive/Colab Notebooks/ASmall_dataset'\n",
        "all_images = os.listdir(path)\n",
        "#print(len(all_images))\n",
        "all_images.sort()\n",
        "data = np.zeros((240,240,155,4))\n",
        "image_data2=np.zeros((240,240,155))\n",
        "train_data=np.zeros((5,128,128,128,4))\n",
        "labels=np.zeros((5,128,128,128,4))\n",
        "\n",
        "for image_num in range(369):\n",
        "    train_label=np.zeros((128,128,128))\n",
        "# data preprocessing starts here\n",
        "    x = all_images[image_num]\n",
        "    print(x)\n",
        "    folder_path = path + '/' + x;\n",
        "    modalities = os.listdir(folder_path)\n",
        "    modalities.sort()\n",
        "    #data = []\n",
        "    w = 0\n",
        "    for j in range(len(modalities)):\n",
        "      #print(modalities[j])\n",
        "      \n",
        "      image_path = folder_path + '/' + modalities[j]\n",
        "      if(image_path[-10:-1] + image_path[-1] == 'seg.nii.gz'):\n",
        "        img = nib.load(image_path);\n",
        "        image_data2 = img.get_data()\n",
        "        image_data2 = np.asarray(image_data2)\n",
        "        print(\"Entered ground truth\")\n",
        "      else:\n",
        "        img = nib.load(image_path);\n",
        "        image_data = img.get_data()\n",
        "        image_data = np.asarray(image_data)\n",
        "        image_data = standardize(image_data)\n",
        "        data[:,:,:,w] = image_data\n",
        "        print(\"Entered modality\",w, image_path[-10:-1] + image_path[-1] )\n",
        "        w = w+1\n",
        "      \n",
        "    print('4 modalities shape',data.shape)\n",
        "    print('label shape',image_data2.shape)  \n",
        "\n",
        "    reshaped_data=data[56:184,80:208,13:141,:]\n",
        "    reshaped_image_data2=image_data2[56:184,80:208,13:141]\n",
        "    print('Cropped 4 modalities shape',reshaped_data.shape)\n",
        "    print('Cropped label shape',reshaped_image_data2.shape)  \n",
        "\n",
        "    \n",
        "    reshaped_data=reshaped_data.reshape(128,128,128,4)\n",
        "    print('reshaped modality', reshaped_data.shape)\n",
        "    train_data[image_num,:,:,:,:] = reshaped_data\n",
        "    # train_data.append(reshaped_data)\n",
        "\n",
        "    reshaped_image_data2=reshaped_image_data2.reshape(128,128,128)\n",
        "    train_label[:,:,:] = reshaped_image_data2\n",
        "    \n",
        "    print('reshaped label ', reshaped_image_data2.shape)\n",
        "    train_label[train_label==4] = 3\n",
        "    print('label4=3',train_label.shape)\n",
        "    hello = train_label.flatten()\n",
        "    print('hello',train_label.shape)\n",
        "\n",
        "        #y_to = keras.utils.to_categorical(y_to,num_classes=2)\n",
        "    print('hello[hello==3]',hello[hello==3].shape)\n",
        "    print(\"Number of classes\",np.unique(hello))\n",
        "\n",
        "    class_weights = class_weight.compute_class_weight('balanced',np.unique(hello),hello)\n",
        "    print('class weights',class_weights)\n",
        "    train_label = tf.keras.utils.to_categorical(train_label, num_classes = 4)\n",
        "    print('labe to categorical',train_label.shape)\n",
        "    print(type(reshaped_data))\n",
        "    # train_label.append(reshaped_image_data2)\n",
        "    labels[image_num,:,:,:,:]=train_label\n",
        "train_data=np.array(train_data)\n",
        "labels=np.array(labels)\n",
        "print(train_data.shape)\n",
        "print(labels.shape)\n",
        "plt.imshow(train_data[1,:,:,80,2])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ybOfpdGmURwl"
      },
      "source": [
        "## **Training**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4fcYmv0dT2kU"
      },
      "source": [
        "\n",
        "model.fit(train_data, labels, batch_size=2, epochs=500)\n",
        "fig, ax = plt.subplots(1, 2, figsize=(20, 5))\n",
        "ax = ax.ravel()\n",
        "\n",
        "for i, metric in enumerate([\"acc\", \"loss\"]):\n",
        "    ax[i].plot(model.history.history[metric])\n",
        "    ax[i].plot(model.history.history[\"val_\" + metric])\n",
        "    ax[i].set_title(\"Model {}\".format(metric))\n",
        "    ax[i].set_xlabel(\"epochs\")\n",
        "    ax[i].set_ylabel(metric)\n",
        "    ax[i].legend([\"train\", \"val\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QQPpAMJcUBts"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G-BHUNV4UMxz"
      },
      "source": [
        "## **Testing**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1BzmVpUwUEwM"
      },
      "source": [
        "\n",
        "import os\n",
        "\n",
        "path = '/content/drive/MyDrive/MICCAI_BraTS2020_ValidationData/'\n",
        "\n",
        "all_images = os.listdir(path)\n",
        "print(len(all_images))\n",
        "all_images.sort()\n",
        "instances=125\n",
        "data = np.zeros((240,240,155,4))\n",
        "chunks_merged=np.zeros((240,240,155))\n",
        "\n",
        "val_data=np.zeros((instances,128,128,128,4))\n",
        "# brats19_prediction='D:/kiran/brats2019/prediction2_basicunet_4/'\n",
        "chunk1= np.zeros((128,128,128,4))\n",
        "chunk2= np.zeros((128,128,128,4))\n",
        "chunk3= np.zeros((128,128,128,4))\n",
        "chunk4= np.zeros((128,128,128,4))\n",
        "\n",
        "\n",
        "for image_num in range(instances):\n",
        "    chunks=[]\n",
        "    print(image_num)\n",
        "    #train_label=np.zeros((128,128,128))\n",
        "    x = all_images[image_num]\n",
        "    \n",
        "    vol_name=x\n",
        "    print('patient',vol_name)\n",
        "    folder_path = path + '/' + x;\n",
        "    modalities = os.listdir(folder_path)\n",
        "    print(modalities)\n",
        "    modalities.sort()\n",
        "    #data = []\n",
        "    w = 0\n",
        "    for j in range(len(modalities)):\n",
        "      print(modalities[j])\n",
        "      image_path = folder_path + '/' + modalities[j]\n",
        "      print(image_path)\n",
        "      img = nib.load(image_path);\n",
        "      image_data = img.get_data()\n",
        "      image_data = np.asarray(image_data)\n",
        "      image_data = standardize(image_data)\n",
        "      data[:,:,:,w] = image_data\n",
        "       #print(\"Entered modality\",w, image_path[-10:-1] + image_path[-1] )\n",
        "      w = w+1\n",
        "    \n",
        "    \n",
        "    print('4 modalities shape',data.shape) \n",
        "   \n",
        "    #divide 240*240*155 sized image to 4 128*128*128 images\n",
        "    chunk1[8:128,8:128,:,:]=data[0:120,0:120,13:141,:] \n",
        "    reshaped_chunk1=chunk1.reshape(128,128,128,4)\n",
        "    plt.imshow(reshaped_chunk1[:,:,80,0])\n",
        "    plt.show()\n",
        "    \n",
        "    chunk2[0:120,8:128,:,:]=data[120:240,0:120,13:141,:] \n",
        "    reshaped_chunk2=chunk2.reshape(128,128,128,4)\n",
        "    plt.imshow(reshaped_chunk2[:,:,80,0])\n",
        "    plt.show()\n",
        "    \n",
        "    chunk3[8:128,0:120,:,:]=data[0:120,120:240,13:141,:] \n",
        "    reshaped_chunk3=chunk3.reshape(128,128,128,4)\n",
        "    plt.imshow(reshaped_chunk3[:,:,80,0])\n",
        "    plt.show()\n",
        "    \n",
        "    chunk4[0:120,0:120,:,:]=data[120:240,120:240,13:141,:]\n",
        "    reshaped_chunk4=chunk4.reshape(128,128,128,4)\n",
        "    plt.imshow(reshaped_chunk4[:,:,80,0])\n",
        "    plt.show()\n",
        "   \n",
        "    #predicting sub patch 1 and adding it back to its place in dummy array\n",
        "    reshaped_chunk1=reshaped_chunk1.reshape(1,128,128,128,4)\n",
        "    prediction=model.predict(reshaped_chunk1)\n",
        "    _classes=prediction.argmax(axis=-1)\n",
        "    print('classes',_classes.shape)\n",
        "    chunks_merged[0:120,0:120,13:141]=_classes[0,8:128,8:128,:]\n",
        "\n",
        "    #predicting sub patch 2 and adding it back to its place in dummy array\n",
        "    reshaped_chunk2=reshaped_chunk2.reshape(1,128,128,128,4)\n",
        "    prediction=model.predict(reshaped_chunk2)\n",
        "    _classes=prediction.argmax(axis=-1)\n",
        "    print('classes',_classes.shape)\n",
        "    chunks_merged[120:240,0:120,13:141]=_classes[0,0:120,8:128,:]\n",
        "\n",
        "    #predicting sub patch 3 and adding it back to its place in dummy array\n",
        "    reshaped_chunk3=reshaped_chunk3.reshape(1,128,128,128,4)\n",
        "    prediction=model.predict(reshaped_chunk3)\n",
        "    _classes=prediction.argmax(axis=-1)\n",
        "    print('classes',_classes.shape)\n",
        "    chunks_merged[0:120,120:240,13:141]=_classes[0,8:128,0:120,:]\n",
        "\n",
        "    #predicting sub patch 4 and adding it back to its place in dummy array\n",
        "    reshaped_chunk4=reshaped_chunk4.reshape(1,128,128,128,4)\n",
        "    prediction=model.predict(reshaped_chunk4)\n",
        "    _classes=prediction.argmax(axis=-1)\n",
        "    print('classes',_classes.shape)\n",
        "    chunks_merged[120:240,120:240,13:141]=_classes[0,0:120,0:120,:]\n",
        "\n",
        "    chunks_merged=chunks_merged[0:240,0:240,:]\n",
        "    #chunks_merged=np.swapaxes(chunks_merged,0,1)\n",
        "    chunks_merged[chunks_merged==3]=4\n",
        "    chunks_merged=chunks_merged.astype('uint8')\n",
        "    plt.imshow(chunks_merged[:,:,80])\n",
        "    plt.show()\n",
        "    # nib.save(nib.Nifti1Image(chunks_merged, None),brats19_prediction+vol_name+'.nii.gz')\n",
        "\n",
        "chunks_merged.shape\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T98BlHJFUe6S"
      },
      "source": [
        "# **Overall Survival-time prediction**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EIJuqSeuUlnP"
      },
      "source": [
        "!pip install SimpleITK\n",
        "!pip install pyradiomics \n",
        "import os\n",
        "import numpy\n",
        "import six\n",
        "import pandas as pd\n",
        "import glob\n",
        "import nibabel as nib\n",
        "import numpy as np\n",
        "from glob import glob\n",
        "# from keras.utils import multi_gpu_model\n",
        "from keras import optimizers\n",
        "import matplotlib.pyplot as plt\n",
        "import collections as clt\n",
        "import SimpleITK as sitk\n",
        "import radiomics\n",
        "# Import the model we are using\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from radiomics import featureextractor, getTestCase, getFeatureClasses\n",
        "from radiomics import firstorder, glcm, imageoperations, shape, glrlm, glszm, getTestCase,gldm,ngtdm\n",
        "import scipy.stats \n",
        "\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.model_selection import train_test_split \n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import mean_squared_error "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZDPtgXqCUwWh"
      },
      "source": [
        "## **Setting**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "csASJMrNUpD7"
      },
      "source": [
        "applyLog = True\n",
        "applyWavelet = True\n",
        "\n",
        "settings = {'binWidth': 25,\n",
        "            'interpolator': sitk.sitkBSpline,\n",
        "            'resampledPixelSpacing': None}\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1t9IVbacU5K4"
      },
      "source": [
        "## **Feature Extraction**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tmt0UsQzUznF"
      },
      "source": [
        "#Training\n",
        "# path_modalities='/content/drive/MyDrive/Colab Notebooks/brats2019/MICCAI_BraTS_2019_Data_Training/HGG'\n",
        "# path_HGG_2019 = glob(path_modalities+'/**')\n",
        "# path_predicted='/content/drive/MyDrive/Colab Notebooks/Survival_prediction_Atrous/357/TrainDataOutput/submission'\n",
        "# path_train_468 = glob(path_predicted+'/**')\n",
        "\n",
        "#Validation\n",
        "path_modalities='/content/drive/MyDrive/Colab Notebooks/brats2019/MICCAI_BraTS_2019_Data_Validation'\n",
        "path_HGG_2019 = glob(path_modalities+'/**')\n",
        "path_predicted='/content/drive/MyDrive/Colab Notebooks/Survival_prediction_Atrous/123/TestDataOutput/submission'\n",
        "path_train_468 = glob(path_predicted+'/**')\n",
        "# mod=''\n",
        "\n",
        "for view in ['axial_','coronal_','sagittal_']:\n",
        "  for modality in ['_t1ce','_t1','_t2','_flair']:\n",
        "    for i in range(len(df['BraTS19ID'])):\n",
        "          print('i',i)\n",
        "          for s in path_HGG_2019:\n",
        "            # print('s',s)\n",
        "            if df['BraTS19ID'][i] in s:\n",
        "              print('@@@@@@@@@@@@@@@@@@@patient',i)\n",
        "              if i != 66:\n",
        "                print('s',s)\n",
        "                if view=='axial_':\n",
        "                  axis1=1\n",
        "                  axis2=2\n",
        "                  mod='axial'+modality+'_'\n",
        "                if view=='coronal_':\n",
        "                  axis1=0\n",
        "                  axis2=1\n",
        "                  mod='coronal'+modality+'_'\n",
        "                if view=='sagittal_':\n",
        "                  axis1=0\n",
        "                  axis2=2\n",
        "                  mod='sagittal'+modality+'_'\n",
        "                image=readData(os.path.join(s+'/'+df['BraTS19ID'][i]+modality+'.nii.gz'),axis1,axis2)\n",
        "                mask=readData(os.path.join(path_predicted+'/'+df['BraTS19ID'][i]+'.nii.gz'),axis1,axis2)\n",
        "                print('image',os.path.join(s+'/'+df['BraTS19ID'][i]+modality+'.nii.gz'))\n",
        "                print('mask', os.path.join(path_predicted+'/'+df['BraTS19ID'][i]+'.nii.gz'))\n",
        "                print('view',view)\n",
        "                print('mod',mod)\n",
        "                print()\n",
        "                #\n",
        "                # If enabled, resample image (resampled image is automatically cropped.\n",
        "                #\n",
        "                interpolator = settings.get('interpolator')\n",
        "                resampledPixelSpacing = settings.get('resampledPixelSpacing')\n",
        "                if interpolator is not None and resampledPixelSpacing is not None:\n",
        "                  image, mask = imageoperations.resampleImage(image, mask, **settings)\n",
        "\n",
        "                bb, correctedMask = imageoperations.checkMask(image, mask)\n",
        "                if correctedMask is not None:\n",
        "                  mask = correctedMask\n",
        "                image, mask = imageoperations.cropToTumorMask(image, mask, bb)\n",
        "                \n",
        "                # #################################################################################################################################################\n",
        "                # Show Shape features\n",
        "                shapeFeatures = shape.RadiomicsShape(image, mask, **settings)\n",
        "                shapeFeatures.enableAllFeatures()\n",
        "                #print('Calculating Shape features...')\n",
        "                results = shapeFeatures.execute()\n",
        "                for (key, val) in six.iteritems(results):\n",
        "                  key=mod+key\n",
        "                  # print('i=',i,' ', key, ':', val)\n",
        "                  df[key][i]=val \n",
        "\n",
        "                firstOrderFeatures = firstorder.RadiomicsFirstOrder(image, mask, **settings)\n",
        "                firstOrderFeatures.enableAllFeatures()\n",
        "                results = firstOrderFeatures.execute()\n",
        "                for (key, val) in six.iteritems(results):\n",
        "                  key=mod+key\n",
        "                  #print('i=',i,'  ', key, ':', val)\n",
        "                  df[key][i]=val \n",
        "                  \n",
        "                \n",
        "                # Show GLCM features\n",
        "                glcmFeatures = glcm.RadiomicsGLCM(image, mask, **settings)\n",
        "                glcmFeatures.enableAllFeatures()\n",
        "                #print('Calculating GLCM features...')\n",
        "                results = glcmFeatures.execute()\n",
        "                for (key, val) in six.iteritems(results):\n",
        "                  key=mod+key\n",
        "                  #print('  ', key, ':', val)\n",
        "                  df[key][i]=val\n",
        "\n",
        "                \n",
        "                # Show GLRLM features\n",
        "                glrlmFeatures = glrlm.RadiomicsGLRLM(image, mask, **settings)\n",
        "                glrlmFeatures.enableAllFeatures()\n",
        "                #print('Calculating GLRLM features...')\n",
        "                results = glrlmFeatures.execute()\n",
        "                for (key, val) in six.iteritems(results):\n",
        "                  key=mod+key\n",
        "                  #print('  ', key, ':', val)\n",
        "                  df[key][i]=val\n",
        "\n",
        "                \n",
        "                # Show GLSZM features\n",
        "                glszmFeatures = glszm.RadiomicsGLSZM(image, mask, **settings)\n",
        "                glszmFeatures.enableAllFeatures()\n",
        "                #print('Calculating GLSZM features...')\n",
        "                results = glszmFeatures.execute()\n",
        "                for (key, val) in six.iteritems(results):\n",
        "                  key=mod+key\n",
        "                  #print('  ', key, ':', val)\n",
        "                  df[key][i]=val\n",
        "                \n",
        "                # Show GLDM features\n",
        "                gldmFeatures = gldm.RadiomicsGLDM(image, mask, **settings)\n",
        "                gldmFeatures.enableAllFeatures()\n",
        "                #print('Calculating GLDM features...')\n",
        "                results = gldmFeatures.execute()\n",
        "                for (key, val) in six.iteritems(results):\n",
        "                  key=mod+key\n",
        "                  #print('  ', key, ':', val)\n",
        "                  df[key][i]=val \n",
        "\n",
        "                # Show NGTDM features\n",
        "                ngtdmFeatures = ngtdm.RadiomicsNGTDM(image, mask, **settings)\n",
        "                ngtdmFeatures.enableAllFeatures()\n",
        "                results = ngtdmFeatures.execute()\n",
        "                #print('Calculated NGTDM features ')\n",
        "                for (key, val) in six.iteritems(results):\n",
        "                    key=mod+key\n",
        "                    #print('  ', key, ':', val)  \n",
        "                    df[key][i]=val\n",
        "                #################################################################################################################################################\n",
        "                #\n",
        "                # \n",
        "                #\n",
        "                if applyLog:\n",
        "                  sigmaValues = numpy.arange(5., 0., -.5)[::1]\n",
        "                  for logImage, imageTypeName, inputKwargs in imageoperations.getLoGImage(image, mask, sigma=sigmaValues):\n",
        "                    \n",
        "                    #Show FirstOrder features, calculated on a LoG filtered image\n",
        "                    logFirstorderFeatures = firstorder.RadiomicsFirstOrder(logImage, mask, **inputKwargs)\n",
        "                    logFirstorderFeatures.enableAllFeatures()\n",
        "                    results = logFirstorderFeatures.execute()\n",
        "                    for (key, val) in six.iteritems(results):\n",
        "                      laplacianFeatureName = '%s_%s' % (imageTypeName, key)\n",
        "                      laplacianFeatureName=mod+laplacianFeatureName\n",
        "                      #print('  ', laplacianFeatureName, ':', val)\n",
        "                      df[laplacianFeatureName][i]=val\n",
        "                    \n",
        "                    \n",
        "                    # Show GLCM  features, calculated on a LoG filtered image\n",
        "                    logglcmFeatures = glcm.RadiomicsGLCM(logImage, mask, **inputKwargs)\n",
        "                    logglcmFeatures.enableAllFeatures()\n",
        "                    results = logglcmFeatures.execute()\n",
        "                    for (key, val) in six.iteritems(results):\n",
        "                      laplacianFeatureName = '%s_%s' % (imageTypeName, key)\n",
        "                      laplacianFeatureName=mod+laplacianFeatureName\n",
        "                      #print('  ', laplacianFeatureName, ':', val)\n",
        "                      df[laplacianFeatureName][i]=val\n",
        "\n",
        "                    \n",
        "                    # Show GLRLM features\n",
        "                    logglrlmFeatures = glrlm.RadiomicsGLRLM(logImage, mask, **inputKwargs)\n",
        "                    logglrlmFeatures.enableAllFeatures()\n",
        "                    #print('Calculating GLRLM features...')\n",
        "                    results = logglrlmFeatures.execute()\n",
        "                    for (key, val) in six.iteritems(results):\n",
        "                      laplacianFeatureName = '%s_%s' % (imageTypeName, key)\n",
        "                      laplacianFeatureName=mod+laplacianFeatureName\n",
        "                      #print('  ', laplacianFeatureName, ':', val)\n",
        "                      df[laplacianFeatureName][i]=val\n",
        "\n",
        "                    \n",
        "                    # Show GLSZM features\n",
        "                    logglszmFeatures = glszm.RadiomicsGLSZM(logImage, mask, **inputKwargs)\n",
        "                    logglszmFeatures.enableAllFeatures()\n",
        "                    #print('Calculating GLSZM features...')\n",
        "                    results = logglszmFeatures.execute()\n",
        "                    for (key, val) in six.iteritems(results):\n",
        "                      laplacianFeatureName = '%s_%s' % (imageTypeName, key)\n",
        "                      laplacianFeatureName=mod+laplacianFeatureName\n",
        "                      #print('  ', laplacianFeatureName, ':', val)\n",
        "                      df[laplacianFeatureName][i]=val\n",
        "                    \n",
        "                    # Show GLDM features\n",
        "                    loggldmFeatures = gldm.RadiomicsGLDM(logImage, mask, **inputKwargs)\n",
        "                    loggldmFeatures.enableAllFeatures()\n",
        "                    #print('Calculating GLDM features...')\n",
        "                    results = loggldmFeatures.execute()\n",
        "                    for (key, val) in six.iteritems(results):\n",
        "                      laplacianFeatureName = '%s_%s' % (imageTypeName, key)\n",
        "                      laplacianFeatureName=mod+laplacianFeatureName\n",
        "                      #print('  ', laplacianFeatureName, ':', val) \n",
        "                      df[laplacianFeatureName][i]=val\n",
        "\n",
        "                    # Show NGTDM features\n",
        "                    logngtdmFeatures = ngtdm.RadiomicsNGTDM(logImage, mask, **inputKwargs)\n",
        "                    logngtdmFeatures.enableAllFeatures()\n",
        "                    results = logngtdmFeatures.execute()\n",
        "                    #print('Calculated NGTDM features ')\n",
        "                    for (key, val) in six.iteritems(results):\n",
        "                      laplacianFeatureName = '%s_%s' % (imageTypeName, key)\n",
        "                      laplacianFeatureName=mod+laplacianFeatureName\n",
        "                      #print('  ', laplacianFeatureName, ':', val)  \n",
        "                      df[laplacianFeatureName][i]=val\n",
        "\n",
        "\n",
        "                #################################################################################################################################################\n",
        "                # Show FirstOrder features, calculated on a wavelet filtered image\n",
        "                #\n",
        "                if applyWavelet:\n",
        "                  for decompositionImage, decompositionName, inputKwargs in imageoperations.getWaveletImage(image, mask):\n",
        "                    waveletFirstOrderFeaturs = firstorder.RadiomicsFirstOrder(decompositionImage, mask, **inputKwargs)\n",
        "                    waveletFirstOrderFeaturs.enableAllFeatures()\n",
        "                    results = waveletFirstOrderFeaturs.execute()\n",
        "                    #print('Calculated firstorder features with wavelet ', decompositionName)\n",
        "                    for (key, val) in six.iteritems(results):\n",
        "                      waveletFeatureName = '%s_%s' % (str(decompositionName), key)\n",
        "                      #print(key)\n",
        "                      waveletFeatureName=mod+waveletFeatureName\n",
        "                      #print('  ', waveletFeatureName, ':', val)\n",
        "                      df[waveletFeatureName][i]=val \n",
        "                  \n",
        "                  for decompositionImage, decompositionName, inputKwargs in imageoperations.getWaveletImage(image, mask):\n",
        "                    waveletglszmFeatures = glszm.RadiomicsGLSZM(decompositionImage, mask, **inputKwargs)\n",
        "                    waveletglszmFeatures.enableAllFeatures()\n",
        "                    results = waveletglszmFeatures.execute()\n",
        "                    #print('Calculated glszm features with wavelet ', decompositionName)\n",
        "                    for (key, val) in six.iteritems(results):\n",
        "                      waveletFeatureName = '%s_%s' % (str(decompositionName), key)\n",
        "                      #print(key)\n",
        "                      waveletFeatureName=mod+waveletFeatureName\n",
        "                      #print('  ', waveletFeatureName, ':', val)\n",
        "                      df[waveletFeatureName][i]=val \n",
        "\n",
        "                for decompositionImage, decompositionName, inputKwargs in imageoperations.getWaveletImage(image, mask):\n",
        "                    waveletglrlmFeatures = glrlm.RadiomicsGLRLM(decompositionImage, mask, **inputKwargs)\n",
        "                    waveletglrlmFeatures.enableAllFeatures()\n",
        "                    results = waveletglrlmFeatures.execute()\n",
        "                    #print('Calculated GLRLM features with wavelet ', decompositionName)\n",
        "                    for (key, val) in six.iteritems(results):\n",
        "                      waveletFeatureName = '%s_%s' % (str(decompositionName), key)\n",
        "                      #print(key)\n",
        "                      waveletFeatureName=mod+waveletFeatureName\n",
        "                      #print('  ', waveletFeatureName, ':', val)\n",
        "                      df[waveletFeatureName][i]=val \n",
        "                for decompositionImage, decompositionName, inputKwargs in imageoperations.getWaveletImage(image, mask):\n",
        "                    waveletglcmFeatures = glcm.RadiomicsGLCM(decompositionImage, mask, **inputKwargs)\n",
        "                    waveletglcmFeatures.enableAllFeatures()\n",
        "                    results = waveletglcmFeatures.execute()\n",
        "                    #print('Calculated GLCM features with wavelet ', decompositionName)\n",
        "                    for (key, val) in six.iteritems(results):\n",
        "                      waveletFeatureName = '%s_%s' % (str(decompositionName), key)\n",
        "                      #print(key)\n",
        "                      waveletFeatureName=mod+waveletFeatureName\n",
        "                      #print('  ', waveletFeatureName, ':', val)\n",
        "                      df[waveletFeatureName][i]=val \n",
        "                for decompositionImage, decompositionName, inputKwargs in imageoperations.getWaveletImage(image, mask):\n",
        "                      waveletgldmFeatures = gldm.RadiomicsGLDM(decompositionImage, mask, **inputKwargs)\n",
        "                      waveletgldmFeatures.enableAllFeatures()\n",
        "                      results = waveletgldmFeatures.execute()\n",
        "                      #print('Calculated GLDM features with wavelet ', decompositionName)\n",
        "                      for (key, val) in six.iteritems(results):\n",
        "                        waveletFeatureName = '%s_%s' % (str(decompositionName), key)\n",
        "                        waveletFeatureName=mod+waveletFeatureName\n",
        "                        #print('  ', waveletFeatureName, ':', val)\n",
        "                        df[waveletFeatureName][i]=val \n",
        "                for decompositionImage, decompositionName, inputKwargs in imageoperations.getWaveletImage(image, mask):\n",
        "                      waveletngtdmFeatures = ngtdm.RadiomicsNGTDM(decompositionImage, mask, **inputKwargs)\n",
        "                      waveletngtdmFeatures.enableAllFeatures()\n",
        "                      results = waveletngtdmFeatures.execute()\n",
        "                      #print('Calculated NGTDM features with wavelet ', decompositionName)\n",
        "                      for (key, val) in six.iteritems(results):\n",
        "                        waveletFeatureName = '%s_%s' % (str(decompositionName), key)\n",
        "                        waveletFeatureName=mod+waveletFeatureName\n",
        "                        #print('  ', waveletFeatureName, ':', val)\n",
        "                        df[waveletFeatureName][i]=val  \n",
        "\n",
        "# 123\n",
        "# 468\n",
        "df.to_csv('/content/drive/MyDrive/Colab Notebooks/Survival_prediction_Atrous/123/validation_all_features_123.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TPeYIG9YVCsU"
      },
      "source": [
        "## **preprocessing**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RuueRPHJU8hL"
      },
      "source": [
        "count=len(df_train['BraTS19ID'])\n",
        "df_train[\"label\"] = \"\"\n",
        "for i in range(count): \n",
        "  if (i!=66): \n",
        "    if(df_train['Survival'][i]>0 and df_train['Survival'][i]<300):\n",
        "      df_train['label'][i]='low'\n",
        "    if(df_train['Survival'][i]>=300 and df_train['Survival'][i]<450):\n",
        "      df_train['label'][i]='medium'\n",
        "    if(df_train['Survival'][i]>=450 ):\n",
        "      df_train['label'][i]='high'\n",
        "df_train"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3MnS8xslVAdc"
      },
      "source": [
        "df_train=df_train.drop([66])\n",
        "df_train=df_train.fillna(0)\n",
        "df_train=df_train.drop(df_train.columns[0],axis=1)\n",
        "df_train=df_train.drop(df_train.columns[0],axis=1)\n",
        "lbl=df_train['label']\n",
        "train_label=df_train['Survival']\n",
        "df_train= df_train.drop(['BraTS19ID','Survival','label'], axis = 1) \n",
        "df_train\n",
        "\n",
        "\n",
        "df_val=df_val.fillna(0)\n",
        "df_val=df_val.drop(df_val.columns[0],axis=1) \n",
        "df_val=df_val.drop(df_val.columns[0],axis=1) \n",
        "val_patients=df_val['BraTS19ID']\n",
        "df_val= df_val.drop(['BraTS19ID'], axis = 1) \n",
        "df_val.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nF_pBo_CVNy7"
      },
      "source": [
        "## **Feature Selection**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fsIFoBddVQ0L"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "X = ff  #independent columns\n",
        "y = lbl  #target column i.e price range\n",
        "from sklearn.ensemble import ExtraTreesClassifier\n",
        "import matplotlib.pyplot as plt\n",
        "model = ExtraTreesClassifier()\n",
        "model.fit(X,y)\n",
        "#print(model.feature_importances_) #use inbuilt class feature_importances of tree based classifiers\n",
        "#plot graph of feature importances for better visualization\n",
        "feat_importances = pd.Series(model.feature_importances_, index=X.columns)\n",
        "print(feat_importances.nlargest(7))\n",
        "feat_importances.nlargest(7).plot(kind='barh')\n",
        "plt.show()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uvMMqKTnVZTd"
      },
      "source": [
        "## **Training and testing**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QJdRlQkeVWDi"
      },
      "source": [
        "features=['axial_t2_log-sigma-3-5-mm-3D_Skewness', 'axial_flair_log-sigma-5-0-mm-3D_SmallAreaHighGrayLevelEmphasis', 'axial_flair_wavelet-LHH_LowGrayLevelZoneEmphasis', 'axial_t2_log-sigma-1-0-mm-3D_SumSquares', 'axial_flair_wavelet-HLH_ShortRunHighGrayLevelEmphasis', 'axial_flair_log-sigma-3-0-mm-3D_Minimum', 'Age'] \n",
        "X_train=df_train[features]\n",
        "X_test=df_val[features]\n",
        "# X_train.head()\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        "regressor=RandomForestRegressor(n_estimators=200,random_state=69)\n",
        "\n",
        "regressor.fit(X_train, train_label)\n",
        "predictions_train = regressor.predict(X_train)\n",
        "predictions_val = regressor.predict(X_test)\n",
        "\n",
        "# pred =  pd.DataFrame(predictions_train)\n",
        "# pred['BraTS19ID']=train_p_id\n",
        "pred2 =  pd.DataFrame(predictions_val)\n",
        "pred2['BraTS19ID']=val_patients\n",
        "\n",
        "# pred2.to_csv('/content/drive/MyDrive/Colab Notebooks/Survival_prediction_Atrous/246/Survival_prediction/Axial_Coronal/ax_co_246_val_6.csv')\n",
        "pred2\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}